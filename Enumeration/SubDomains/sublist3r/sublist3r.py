#!/usr/bin/env python


import contextlib as __stickytape_contextlib

@__stickytape_contextlib.contextmanager
def __stickytape_temporary_dir():
    import tempfile
    import shutil
    dir_path = tempfile.mkdtemp()
    try:
        yield dir_path
    finally:
        shutil.rmtree(dir_path)

with __stickytape_temporary_dir() as __stickytape_working_dir:
    def __stickytape_write_module(path, contents):
        import os, os.path, errno

        def make_package(path):
            parts = path.split("/")
            partial_path = __stickytape_working_dir
            for part in parts:
                partial_path = os.path.join(partial_path, part)
                if not os.path.exists(partial_path):
                    os.mkdir(partial_path)
                    open(os.path.join(partial_path, "__init__.py"), "w").write("\n")
                    
        make_package(os.path.dirname(path))
        
        full_path = os.path.join(__stickytape_working_dir, path)
        with open(full_path, "w") as module_file:
            module_file.write(contents)

    import sys as __stickytape_sys
    __stickytape_sys.path.insert(0, __stickytape_working_dir)

    __stickytape_write_module('''subbrute/__init__.py''', '''''')
    __stickytape_write_module('''subbrute/subbrute.py''', '''#!/usr/bin/env python\n#\n#SubBrute v1.2\n#A (very) fast subdomain enumeration tool.\n#\n#Maintained by rook\n#Contributors:\n#JordanMilne, KxCode, rc0r, memoryprint, ppaulojr  \n#\nimport re\nimport optparse\nimport os\nimport signal\nimport sys\nimport uuid\nimport random\nimport ctypes\nimport dns.resolver\nimport dns.rdatatype\nimport json\n\n#Python 2.x and 3.x compatiablity\n#We need the Queue library for exception handling\ntry:\n    import queue as Queue\nexcept:\n    import Queue\n\n#The 'multiprocessing' library does not rely upon a Global Interpreter Lock (GIL)\nimport multiprocessing\n\n#Microsoft compatiablity\nif  sys.platform.startswith('win'):\n    #Drop-in replacement,  subbrute + multiprocessing throws exceptions on windows.\n    import threading\n    multiprocessing.Process = threading.Thread\n\nclass verify_nameservers(multiprocessing.Process):\n\n    def __init__(self, target, record_type, resolver_q, resolver_list, wildcards):\n        multiprocessing.Process.__init__(self, target = self.run)\n        self.daemon = True\n        signal_init()\n\n        self.time_to_die = False\n        self.resolver_q = resolver_q\n        self.wildcards = wildcards\n        #Do we need wildcards for other types of records?\n        #This needs testing!\n        self.record_type = "A"\n        if record_type == "AAAA":\n            self.record_type = record_type\n        self.resolver_list = resolver_list\n        resolver = dns.resolver.Resolver()\n        #The domain provided by the user.\n        self.target = target\n        #1 website in the world,  modify the following line when this status changes.\n        #www.google.cn,  I'm looking at you ;)\n        self.most_popular_website = "www.google.com"\n        #We shouldn't need the backup_resolver, but we we can use them if need be.\n        #We must have a resolver,  and localhost can work in some environments.\n        self.backup_resolver = resolver.nameservers + ['127.0.0.1', '8.8.8.8', '8.8.4.4']\n        #Ideally a nameserver should respond in less than 1 sec.\n        resolver.timeout = 1\n        resolver.lifetime = 1\n        try:\n            #Lets test the letancy of our connection.\n            #Google's DNS server should be an ideal time test.\n            resolver.nameservers = ['8.8.8.8']\n            resolver.query(self.most_popular_website, self.record_type)\n        except:\n            #Our connection is slower than a junebug in molasses\n            resolver = dns.resolver.Resolver()\n        self.resolver = resolver\n\n    def end(self):\n        self.time_to_die = True\n\n    #This process cannot block forever,  it  needs to check if its time to die.\n    def add_nameserver(self, nameserver):\n        keep_trying = True\n        while not self.time_to_die and keep_trying:\n            try:\n                self.resolver_q.put(nameserver, timeout = 1)\n                trace("Added nameserver:", nameserver)\n                keep_trying = False\n            except Exception as e:\n                if type(e) == Queue.Full or str(type(e)) == "<class 'queue.Full'>":\n                    keep_trying = True\n\n    def verify(self, nameserver_list):\n        added_resolver = False\n        for server in nameserver_list:\n            if self.time_to_die:\n                #We are done here.\n                break\n            server = server.strip()\n            if server:\n                self.resolver.nameservers = [server]\n                try:\n                    #test_result = self.resolver.query(self.most_popular_website, "A")\n                    #should throw an exception before this line.\n                    if True:#test_result:\n                        #Only add the nameserver to the queue if we can detect wildcards. \n                        if(self.find_wildcards(self.target)):# and self.find_wildcards(".com")\n                            #wildcards have been added to the set, it is now safe to be added to the queue.\n                            #blocking queue,  this process will halt on put() when the queue is full:\n                            self.add_nameserver(server)\n                            added_resolver = True\n                        else:\n                            trace("Rejected nameserver - wildcard:", server)\n                except Exception as e:\n                    #Rejected server :(\n                    trace("Rejected nameserver - unreliable:", server, type(e)) \n        return added_resolver\n\n    def run(self):\n        #Every user will get a different set of resovlers, this helps redistribute traffic.\n        random.shuffle(self.resolver_list)\n        if not self.verify(self.resolver_list):\n            #This should never happen,  inform the user.\n            sys.stderr.write('Warning: No nameservers found, trying fallback list.\\n')\n            #Try and fix it for the user:\n            self.verify(self.backup_resolver)\n        #End of the resolvers list.\n        try:\n            self.resolver_q.put(False, timeout = 1)\n        except:\n            pass\n\n    #Only add the nameserver to the queue if we can detect wildcards. \n    #Returns False on error.\n    def find_wildcards(self, host):\n        #We want sovle the following three problems:\n        #1)The target might have a wildcard DNS record.\n        #2)The target maybe using geolocaiton-aware DNS.\n        #3)The DNS server we are testing may respond to non-exsistant 'A' records with advertizements.\n        #I have seen a CloudFlare Enterprise customer with the first two conditions.\n        try:\n            #This is case #3,  these spam nameservers seem to be more trouble then they are worth.\n             wildtest = self.resolver.query(uuid.uuid4().hex + ".com", "A")\n             if len(wildtest):\n                trace("Spam DNS detected:", host)\n                return False\n        except:\n            pass\n        test_counter = 8\n        looking_for_wildcards = True\n        while looking_for_wildcards and test_counter >= 0 :\n            looking_for_wildcards = False\n            #Don't get lost, this nameserver could be playing tricks.\n            test_counter -= 1            \n            try:\n                testdomain = "%s.%s" % (uuid.uuid4().hex, host)\n                wildtest = self.resolver.query(testdomain, self.record_type)\n                #This 'A' record may contain a list of wildcards.\n                if wildtest:\n                    for w in wildtest:\n                        w = str(w)\n                        if w not in self.wildcards:\n                            #wildcards were detected.\n                            self.wildcards[w] = None\n                            #We found atleast one wildcard, look for more.\n                            looking_for_wildcards = True\n            except Exception as e:\n                if type(e) == dns.resolver.NXDOMAIN or type(e) == dns.name.EmptyLabel:\n                    #not found\n                    return True\n                else:\n                    #This resolver maybe flakey, we don't want it for our tests.\n                    trace("wildcard exception:", self.resolver.nameservers, type(e)) \n                    return False \n        #If we hit the end of our depth counter and,\n        #there are still wildcards, then reject this nameserver because it smells bad.\n        return (test_counter >= 0)\n\nclass lookup(multiprocessing.Process):\n\n    def __init__(self, in_q, out_q, resolver_q, domain, wildcards, spider_blacklist):\n        multiprocessing.Process.__init__(self, target = self.run)\n        signal_init()\n        self.required_nameservers = 16\n        self.in_q = in_q\n        self.out_q = out_q\n        self.resolver_q = resolver_q        \n        self.domain = domain\n        self.wildcards = wildcards\n        self.spider_blacklist = spider_blacklist\n        self.resolver = dns.resolver.Resolver()\n        #Force pydns to use our nameservers\n        self.resolver.nameservers = []\n\n    def get_ns(self):\n        ret = []\n        try:\n            ret = [self.resolver_q.get_nowait()]\n            if ret == False:\n                #Queue is empty,  inform the rest.\n                self.resolver_q.put(False)\n                ret = []\n        except:\n            pass      \n        return ret  \n\n    def get_ns_blocking(self):\n        ret = []\n        ret = [self.resolver_q.get()]\n        if ret == False:\n            trace("get_ns_blocking - Resolver list is empty.")\n            #Queue is empty,  inform the rest.\n            self.resolver_q.put(False)\n            ret = []\n        return ret\n\n    def check(self, host, record_type = "A", retries = 0):\n        trace("Checking:", host)\n        cname_record = []\n        retries = 0        \n        if len(self.resolver.nameservers) <= self.required_nameservers:\n            #This process needs more nameservers,  lets see if we have one avaible\n            self.resolver.nameservers += self.get_ns()\n        #Ok we should be good to go.\n        while True:\n            try:\n                #Query the nameserver, this is not simple...\n                if not record_type or record_type == "A":\n                    resp = self.resolver.query(host)\n                    #Crawl the response\n                    hosts = extract_hosts(str(resp.response), self.domain)\n                    for h in hosts:\n                        if h not in self.spider_blacklist:\n                            self.spider_blacklist[h]=None\n                            trace("Found host with spider:", h)\n                            self.in_q.put((h, record_type, 0))\n                    return resp\n                if record_type == "CNAME":\n                    #A max 20 lookups\n                    for x in range(20):\n                        try:\n                            resp = self.resolver.query(host, record_type)\n                        except dns.resolver.NoAnswer:\n                            resp = False\n                            pass\n                        if resp and resp[0]:\n                            host = str(resp[0]).rstrip(".")\n                            cname_record.append(host)\n                        else:\n                            return cname_record                    \n                else:\n                    #All other records:\n                    return self.resolver.query(host, record_type)\n\n            except Exception as e:\n                if type(e) == dns.resolver.NoNameservers:\n                    #We should never be here.\n                    #We must block,  another process should try this host.\n                    #do we need a limit?\n                    self.in_q.put((host, record_type, 0))\n                    self.resolver.nameservers += self.get_ns_blocking()\n                    return False\n                elif type(e) == dns.resolver.NXDOMAIN:\n                    #"Non-existent domain name."\n                    return False\n                elif type(e) == dns.resolver.NoAnswer:\n                    #"The response did not contain an answer."\n                    if retries >= 1:\n                        trace("NoAnswer retry")\n                        return False\n                    retries += 1\n                elif type(e) == dns.resolver.Timeout:\n                    trace("lookup failure:", host, retries)\n                    #Check if it is time to give up.\n                    if retries >= 3:\n                        if retries > 3:\n                            #Sometimes 'internal use' subdomains will timeout for every request.\n                            #As far as I'm concerned, the authorative name server has told us this domain exists,\n                            #we just can't know the address value using this method.\n                            return ['Mutiple Query Timeout - External address resolution was restricted']\n                        else:\n                            #Maybe another process can take a crack at it.\n                            self.in_q.put((host, record_type, retries + 1))\n                        return False\n                    retries += 1\n                    #retry...\n                elif type(e) == IndexError:\n                    #Some old versions of dnspython throw this error,\n                    #doesn't seem to affect the results,  and it was fixed in later versions.\n                    pass\n                elif type(e) == TypeError:\n                    # We'll get here if the number procs > number of resolvers.\n                    # This is an internal error do we need a limit?\n                    self.in_q.put((host, record_type, 0))\n                    return False\n                elif type(e) == dns.rdatatype.UnknownRdatatype:\n                    error("DNS record type not supported:", record_type)\n                else:\n                    trace("Problem processing host:", host)\n                    #dnspython threw some strange exception...\n                    raise e\n\n    def run(self):\n        #This process needs one resolver before it can start looking.\n        self.resolver.nameservers += self.get_ns_blocking()\n        while True:\n            found_addresses = []\n            work = self.in_q.get()\n            #Check if we have hit the end marker\n            while not work:\n                #Look for a re-queued lookup\n                try:\n                    work = self.in_q.get(blocking = False)\n                    #if we took the end marker of the queue we need to put it back\n                    if work:\n                        self.in_q.put(False)\n                except:#Queue.Empty\n                    trace('End of work queue')\n                    #There isn't an item behind the end marker\n                    work = False\n                    break\n            #Is this the end all work that needs to be done?\n            if not work:\n                #Perpetuate the end marker for all threads to see\n                self.in_q.put(False)\n                #Notify the parent that we have died of natural causes\n                self.out_q.put(False)\n                break\n            else:\n                if len(work) == 3:\n                    #keep track of how many times this lookup has timedout.\n                    (hostname, record_type, timeout_retries) = work\n                    response = self.check(hostname, record_type, timeout_retries)\n                else:\n                    (hostname, record_type) = work\n                    response = self.check(hostname, record_type) \n                sys.stdout.flush()\n                trace(response)                  \n                #self.wildcards is populated by the verify_nameservers() thread.\n                #This variable doesn't need a muetex, because it has a queue. \n                #A queue ensure nameserver cannot be used before it's wildcard entries are found.\n                reject = False\n                if response:\n                    for a in response:\n                        a = str(a)\n                        if a in self.wildcards:\n                            trace("resovled wildcard:", hostname)\n                            reject= True\n                            #reject this domain.\n                            break;\n                        else:\n                            found_addresses.append(a)\n                    if not reject:\n                        #This request is filled, send the results back  \n                        result = (hostname, record_type, found_addresses)\n                        self.out_q.put(result)\n\n#Extract relevant hosts\n#The dot at the end of a domain signifies the root,\n#and all TLDs are subs of the root.\nhost_match = re.compile(r"((?<=[\\s])[a-zA-Z0-9_-]+\\.(?:[a-zA-Z0-9_-]+\\.?)+(?=[\\s]))")\ndef extract_hosts(data, hostname):\n    #made a global to avoid re-compilation\n    global host_match\n    ret = []\n    hosts = re.findall(host_match, data)\n    for fh in hosts:\n        host = fh.rstrip(".")\n        #Is this host in scope?\n        if host.endswith(hostname):\n            ret.append(host)\n    return ret\n\n#Return a list of unique sub domains,  sorted by frequency.\n#Only match domains that have 3 or more sections subdomain.domain.tld\ndomain_match = re.compile("([a-zA-Z0-9_-]*\\.[a-zA-Z0-9_-]*\\.[a-zA-Z0-9_-]*)+")\ndef extract_subdomains(file_name):\n    #Avoid re-compilation\n    global domain_match\n    subs = {}\n    sub_file = open(file_name).read()\n    f_all = re.findall(domain_match, sub_file)\n    del sub_file\n    for i in f_all:\n        if i.find(".") >= 0:\n            p = i.split(".")[0:-1]\n            #gobble everything that might be a TLD\n            while p and len(p[-1]) <= 3:\n                p = p[0:-1]\n            #remove the domain name\n            p = p[0:-1]\n            #do we have a subdomain.domain left?\n            if len(p) >= 1:\n                trace(str(p), " : ", i)\n                for q in p:\n                    if q :\n                        #domain names can only be lower case.\n                        q = q.lower()\n                        if q in subs:\n                            subs[q] += 1\n                        else:\n                            subs[q] = 1\n    #Free some memory before the sort...\n    del f_all\n    #Sort by freq in desc order\n    subs_sorted = sorted(subs.keys(), key = lambda x: subs[x], reverse = True)\n    return subs_sorted\n\ndef print_target(target, record_type = None, subdomains = "names.txt", resolve_list = "resolvers.txt", process_count = 16, output = False, json_output = False, found_subdomains=[],verbose=False):\n    subdomains_list = []\n    results_temp = []\n    run(target, record_type, subdomains, resolve_list, process_count)\n    for result in run(target, record_type, subdomains, resolve_list, process_count):\n        (hostname, record_type, response) = result\n        if not record_type:\n            result = hostname\n        else:\n            result = "%s,%s" % (hostname, ",".join(response).strip(","))\n        if result not in found_subdomains:\n            if verbose:\n                print(result)\n            subdomains_list.append(result)\n\n    return  set(subdomains_list)\n\ndef run(target, record_type = None, subdomains = "names.txt", resolve_list = "resolvers.txt", process_count = 16):\n    subdomains = check_open(subdomains)\n    resolve_list = check_open(resolve_list)\n    if (len(resolve_list) / 16) < process_count:\n        sys.stderr.write('Warning: Fewer than 16 resovlers per thread, consider adding more nameservers to resolvers.txt.\\n')\n    if os.name == 'nt':\n        wildcards = {}\n        spider_blacklist = {}\n    else:\n        wildcards = multiprocessing.Manager().dict()\n        spider_blacklist = multiprocessing.Manager().dict()\n    in_q = multiprocessing.Queue()\n    out_q = multiprocessing.Queue()\n    #have a buffer of at most two new nameservers that lookup processes can draw from.\n    resolve_q = multiprocessing.Queue(maxsize = 2)\n\n    #Make a source of fast nameservers avaiable for other processes.\n    verify_nameservers_proc = verify_nameservers(target, record_type, resolve_q, resolve_list, wildcards)\n    verify_nameservers_proc.start()\n    #The empty string \n    in_q.put((target, record_type))\n    spider_blacklist[target]=None\n    #A list of subdomains is the input\n    for s in subdomains:\n        s = str(s).strip()\n        if s:\n            if s.find(","):\n                #SubBrute should be forgiving, a comma will never be in a url\n                #but the user might try an use a CSV file as input.\n                s=s.split(",")[0]\n            if not s.endswith(target):\n                hostname = "%s.%s" % (s, target)\n            else:\n                #A user might feed an output list as a subdomain list.\n                hostname = s\n            if hostname not in spider_blacklist:\n                spider_blacklist[hostname]=None\n                work = (hostname, record_type)\n                in_q.put(work)\n    #Terminate the queue\n    in_q.put(False)\n    for i in range(process_count):\n        worker = lookup(in_q, out_q, resolve_q, target, wildcards, spider_blacklist)\n        worker.start()\n    threads_remaining = process_count\n    while True:\n        try:\n            #The output is valid hostnames\n            result = out_q.get(True, 10)\n            #we will get an empty exception before this runs. \n            if not result:\n                threads_remaining -= 1\n            else:\n                #run() is a generator, and yields results from the work queue\n                yield result\n        except Exception as e:\n            #The cx_freeze version uses queue.Empty instead of Queue.Empty :(\n            if type(e) == Queue.Empty or str(type(e)) == "<class 'queue.Empty'>":\n                pass\n            else:\n                raise(e)\n        #make sure everyone is complete\n        if threads_remaining <= 0:\n            break\n    trace("killing nameserver process")\n    #We no longer require name servers.\n    try:\n        killproc(pid = verify_nameservers_proc.pid)\n    except:\n        #Windows threading.tread\n        verify_nameservers_proc.end()\n    trace("End")\n\n#exit handler for signals.  So ctrl+c will work. \n#The 'multiprocessing' library each process is it's own process which side-steps the GIL\n#If the user wants to exit prematurely,  each process must be killed.\ndef killproc(signum = 0, frame = 0, pid = False):\n    if not pid:\n        pid = os.getpid()\n    if sys.platform.startswith('win'):\n        try:\n            kernel32 = ctypes.windll.kernel32\n            handle = kernel32.OpenProcess(1, 0, pid)\n            kernel32.TerminateProcess(handle, 0)\n        except:\n            #Oah windows.\n            pass\n    else:\n        os.kill(pid, 9)\n\n#Toggle debug output\nverbose = False\ndef trace(*args, **kwargs):\n    if verbose:\n        for a in args:\n            sys.stderr.write(str(a))\n            sys.stderr.write(" ")\n        sys.stderr.write("\\n")\n\ndef error(*args, **kwargs):\n    for a in args:\n        sys.stderr.write(str(a))\n        sys.stderr.write(" ")\n    sys.stderr.write("\\n")\n    sys.exit(1)\n\ndef check_open(input_file):\n    ret = []\n    #If we can't find a resolver from an input file, then we need to improvise.\n    try:\n        ret = open(input_file).readlines()\n    except:\n        error("File not found:", input_file)\n    if not len(ret):\n        error("File is empty:", input_file)\n    return ret\n\n#Every 'multiprocessing' process needs a signal handler.\n#All processes need to die, we don't want to leave zombies.\ndef signal_init():\n    #Escliate signal to prevent zombies.\n    signal.signal(signal.SIGINT, killproc)\n    try:\n        signal.signal(signal.SIGTSTP, killproc)\n        signal.signal(signal.SIGQUIT, killproc)\n    except:\n        #Windows\n        pass\n\nif __name__ == "__main__":\n    if getattr(sys, 'frozen', False):\n        # cx_freeze windows:\n        base_path = os.path.dirname(sys.executable)\n        multiprocessing.freeze_support()\n    else:\n        #everything else:\n        base_path = os.path.dirname(os.path.realpath(__file__))\n    parser = optparse.OptionParser("usage: %prog [options] target")\n    parser.add_option("-s", "--subs", dest = "subs", default = os.path.join(base_path, "names.txt"),\n              type = "string", help = "(optional) list of subdomains,  default = 'names.txt'")\n    parser.add_option("-r", "--resolvers", dest = "resolvers", default = os.path.join(base_path, "resolvers.txt"),\n              type = "string", help = "(optional) A list of DNS resolvers, if this list is empty it will OS's internal resolver default = 'resolvers.txt'")\n    parser.add_option("-t", "--targets_file", dest = "targets", default = "",\n              type = "string", help = "(optional) A file containing a newline delimited list of domains to brute force.")\n    parser.add_option("-o", "--output", dest = "output",  default = False, help = "(optional) Output to file (Greppable Format)")\n    parser.add_option("-j", "--json", dest="json", default = False, help="(optional) Output to file (JSON Format)")\n    parser.add_option("-a", "-A", action = 'store_true', dest = "ipv4", default = False,\n              help = "(optional) Print all IPv4 addresses for sub domains (default = off).")\n    parser.add_option("--type", dest = "type", default = False,\n              type = "string", help = "(optional) Print all reponses for an arbitrary DNS record type (CNAME, AAAA, TXT, SOA, MX...)")                  \n    parser.add_option("-c", "--process_count", dest = "process_count",\n              default = 16, type = "int",\n              help = "(optional) Number of lookup theads to run. default = 16")\n    parser.add_option("-f", "--filter_subs", dest = "filter", default = "",\n              type = "string", help = "(optional) A file containing unorganized domain names which will be filtered into a list of subdomains sorted by frequency.  This was used to build names.txt.")                 \n    parser.add_option("-v", "--verbose", action = 'store_true', dest = "verbose", default = False,\n              help = "(optional) Print debug information.")\n    (options, args) = parser.parse_args()\n\n    \n    verbose = options.verbose\n\n    if len(args) < 1 and options.filter == "" and options.targets == "":\n        parser.error("You must provie a target. Use -h for help.")\n\n    if options.filter != "":\n        #cleanup this file and print it out\n        for d in extract_subdomains(options.filter):\n            print(d)\n        sys.exit()\n\n    if options.targets != "":\n        targets = check_open(options.targets) #the domains\n    else:\n        targets = args #multiple arguments on the cli: ./subbrute.py google.com gmail.com yahoo.com    if (len(resolver_list) / 16) < options.process_count:\n\n    output = False\n    if options.output:\n        try:\n             output = open(options.output, "w")\n        except:\n            error("Failed writing to file:", options.output)\n\n    json_output = False\n    if options.json:\n        try:\n            json_output = open(options.json, "w")\n        except:\n            error("Failed writing to file:", options.json)\n\n    record_type = False\n    if options.ipv4:\n        record_type="A"\n    if options.type:\n        record_type = str(options.type).upper()\n\n    threads = []\n    for target in targets:\n        target = target.strip()\n        if target:\n\n            #target => domain\n            #record_type => \n            #options.subs => file the contain the subdomains list\n            #options.process_count => process count default = 16\n            #options.resolvers => the resolvers file\n            #options.output\n            #options.json\n            print(target, record_type, options.subs, options.resolvers, options.process_count, output, json_output)\n            print_target(target, record_type, options.subs, options.resolvers, options.process_count, output, json_output)\n\n\n''')
    #!/usr/bin/env python
    # coding: utf-8
    # Sublist3r v1.0
    # By Ahmed Aboul-Ela - twitter.com/aboul3la
    
    # modules in standard library
    import re
    import sys
    import os
    import argparse
    import time
    import hashlib
    import random
    import multiprocessing
    import threading
    import socket
    import json
    from collections import Counter
    
    # external modules
    from subbrute import subbrute
    import dns.resolver
    import requests
    
    # Python 2.x and 3.x compatiablity
    if sys.version > '3':
        import urllib.parse as urlparse
        import urllib.parse as urllib
    else:
        import urlparse
        import urllib
    
    # In case you cannot install some of the required development packages
    # there's also an option to disable the SSL warning:
    try:
        import requests.packages.urllib3
        requests.packages.urllib3.disable_warnings()
    except:
        pass
    
    # Check if we are running this on windows platform
    is_windows = sys.platform.startswith('win')
    
    # Console Colors
    if is_windows:
        # Windows deserves coloring too :D
        G = '\033[92m'  # green
        Y = '\033[93m'  # yellow
        B = '\033[94m'  # blue
        R = '\033[91m'  # red
        W = '\033[0m'   # white
        try:
            import win_unicode_console , colorama
            win_unicode_console.enable()
            colorama.init()
            #Now the unicode will work ^_^
        except:
            print("[!] Error: Coloring libraries not installed, no coloring will be used [Check the readme]")
            G = Y = B = R = W = G = Y = B = R = W = ''
    
    
    else:
        G = '\033[92m'  # green
        Y = '\033[93m'  # yellow
        B = '\033[94m'  # blue
        R = '\033[91m'  # red
        W = '\033[0m'   # white
    
    
    def banner():
        print("""%s
                     ____        _     _ _     _   _____
                    / ___| _   _| |__ | (_)___| |_|___ / _ __
                    \___ \| | | | '_ \| | / __| __| |_ \| '__|
                     ___) | |_| | |_) | | \__ \ |_ ___) | |
                    |____/ \__,_|_.__/|_|_|___/\__|____/|_|%s%s
    
                    # Coded By Ahmed Aboul-Ela - @aboul3la
        """ % (R, W, Y))
    
    
    def parser_error(errmsg):
        banner()
        print("Usage: python " + sys.argv[0] + " [Options] use -h for help")
        print(R + "Error: " + errmsg + W)
        sys.exit()
    
    
    def parse_args():
        # parse the arguments
        parser = argparse.ArgumentParser(epilog='\tExample: \r\npython ' + sys.argv[0] + " -d google.com")
        parser.error = parser_error
        parser._optionals.title = "OPTIONS"
        parser.add_argument('-d', '--domain', help="Domain name to enumerate it's subdomains", required=True)
        parser.add_argument('-b', '--bruteforce', help='Enable the subbrute bruteforce module', nargs='?', default=False)
        parser.add_argument('-p', '--ports', help='Scan the found subdomains against specified tcp ports')
        parser.add_argument('-v', '--verbose', help='Enable Verbosity and display results in realtime', nargs='?', default=False)
        parser.add_argument('-t', '--threads', help='Number of threads to use for subbrute bruteforce', type=int, default=30)
        parser.add_argument('-e', '--engines', help='Specify a comma-separated list of search engines')
        parser.add_argument('-o', '--output', help='Save the results to text file')
        return parser.parse_args()
    
    
    def write_file(filename, subdomains):
        # saving subdomains results to output file
        print("%s[-] Saving results to file: %s%s%s%s" % (Y, W, R, filename, W))
        with open(str(filename), 'wt') as f:
            for subdomain in subdomains:
                f.write(subdomain + os.linesep)
    
    
    def subdomain_sorting_key(hostname):
        """Sorting key for subdomains
    
        This sorting key orders subdomains from the top-level domain at the right
        reading left, then moving '^' and 'www' to the top of their group. For
        example, the following list is sorted correctly:
    
        [
            'example.com',
            'www.example.com',
            'a.example.com',
            'www.a.example.com',
            'b.a.example.com',
            'b.example.com',
            'example.net',
            'www.example.net',
            'a.example.net',
        ]
    
        """
        parts = hostname.split('.')[::-1]
        if parts[-1] == 'www':
            return parts[:-1], 1
        return parts, 0
    
    
    class enumratorBase(object):
        def __init__(self, base_url, engine_name, domain, subdomains=None, silent=False, verbose=True):
            subdomains = subdomains or []
            self.domain = urlparse.urlparse(domain).netloc
            self.session = requests.Session()
            self.subdomains = []
            self.timeout = 25
            self.base_url = base_url
            self.engine_name = engine_name
            self.silent = silent
            self.verbose = verbose
            self.headers = {
                  'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36',
                  'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                  'Accept-Language': 'en-US,en;q=0.8',
                  'Accept-Encoding': 'gzip',
              }
            self.print_banner()
    
        def print_(self, text):
            if not self.silent:
                print(text)
            return
    
        def print_banner(self):
            """ subclass can override this if they want a fancy banner :)"""
            self.print_(G + "[-] Searching now in %s.." % (self.engine_name) + W)
            return
    
        def send_req(self, query, page_no=1):
    
            url = self.base_url.format(query=query, page_no=page_no)
            try:
                resp = self.session.get(url, headers=self.headers, timeout=self.timeout)
            except Exception:
                resp = None
            return self.get_response(resp)
    
        def get_response(self, response):
            if response is None:
                return 0
            return response.text if hasattr(response, "text") else response.content
    
        def check_max_subdomains(self, count):
            if self.MAX_DOMAINS == 0:
                return False
            return count >= self.MAX_DOMAINS
    
        def check_max_pages(self, num):
            if self.MAX_PAGES == 0:
                return False
            return num >= self.MAX_PAGES
    
        # override
        def extract_domains(self, resp):
            """ chlid class should override this function """
            return
    
        # override
        def check_response_errors(self, resp):
            """ chlid class should override this function
            The function should return True if there are no errors and False otherwise
            """
            return True
    
        def should_sleep(self):
            """Some enumrators require sleeping to avoid bot detections like Google enumerator"""
            return
    
        def generate_query(self):
            """ chlid class should override this function """
            return
    
        def get_page(self, num):
            """ chlid class that user different pagnation counter should override this function """
            return num + 10
    
        def enumerate(self, altquery=False):
            flag = True
            page_no = 0
            prev_links = []
            retries = 0
    
            while flag:
                query = self.generate_query()
                count = query.count(self.domain)  # finding the number of subdomains found so far
    
                # if they we reached the maximum number of subdomains in search query
                # then we should go over the pages
                if self.check_max_subdomains(count):
                    page_no = self.get_page(page_no)
    
                if self.check_max_pages(page_no):  # maximum pages for Google to avoid getting blocked
                    return self.subdomains
                resp = self.send_req(query, page_no)
    
                # check if there is any error occured
                if not self.check_response_errors(resp):
                    return self.subdomains
                links = self.extract_domains(resp)
    
                # if the previous page hyperlinks was the similar to the current one, then maybe we have reached the last page
                if links == prev_links:
                    retries += 1
                    page_no = self.get_page(page_no)
    
            # make another retry maybe it isn't the last page
                    if retries >= 3:
                        return self.subdomains
    
                prev_links = links
                self.should_sleep()
    
            return self.subdomains
    
    
    class enumratorBaseThreaded(multiprocessing.Process, enumratorBase):
        def __init__(self, base_url, engine_name, domain, subdomains=None, q=None, lock=threading.Lock(), silent=False, verbose=True):
            subdomains = subdomains or []
            enumratorBase.__init__(self, base_url, engine_name, domain, subdomains, silent=silent, verbose=verbose)
            multiprocessing.Process.__init__(self)
            self.lock = lock
            self.q = q
            return
    
        def run(self):
            domain_list = self.enumerate()
            for domain in domain_list:
                self.q.append(domain)
    
    
    class GoogleEnum(enumratorBaseThreaded):
        def __init__(self, domain, subdomains=None, q=None, silent=False, verbose=True):
            subdomains = subdomains or []
            base_url = "https://google.com/search?q={query}&btnG=Search&hl=en-US&biw=&bih=&gbv=1&start={page_no}&filter=0"
            self.engine_name = "Google"
            self.MAX_DOMAINS = 11
            self.MAX_PAGES = 200
            super(GoogleEnum, self).__init__(base_url, self.engine_name, domain, subdomains, q=q, silent=silent, verbose=verbose)
            self.q = q
            return
    
        def extract_domains(self, resp):
            links_list = list()
            link_regx = re.compile('<cite.*?>(.*?)<\/cite>')
            try:
                links_list = link_regx.findall(resp)
                for link in links_list:
                    link = re.sub('<span.*>', '', link)
                    if not link.startswith('http'):
                        link = "http://" + link
                    subdomain = urlparse.urlparse(link).netloc
                    if subdomain and subdomain not in self.subdomains and subdomain != self.domain:
                        if self.verbose:
                            self.print_("%s%s: %s%s" % (R, self.engine_name, W, subdomain))
                        self.subdomains.append(subdomain.strip())
            except Exception:
                pass
            return links_list
    
        def check_response_errors(self, resp):
            if (type(resp) is str or type(resp) is unicode) and 'Our systems have detected unusual traffic' in resp:
                self.print_(R + "[!] Error: Google probably now is blocking our requests" + W)
                self.print_(R + "[~] Finished now the Google Enumeration ..." + W)
                return False
            return True
    
        def should_sleep(self):
            time.sleep(5)
            return
    
        def generate_query(self):
            if self.subdomains:
                fmt = 'site:{domain} -www.{domain} -{found}'
                found = ' -'.join(self.subdomains[:self.MAX_DOMAINS - 2])
                query = fmt.format(domain=self.domain, found=found)
            else:
                query = "site:{domain} -www.{domain}".format(domain=self.domain)
            return query
    
    
    class YahooEnum(enumratorBaseThreaded):
        def __init__(self, domain, subdomains=None, q=None, silent=False, verbose=True):
            subdomains = subdomains or []
            base_url = "https://search.yahoo.com/search?p={query}&b={page_no}"
            self.engine_name = "Yahoo"
            self.MAX_DOMAINS = 10
            self.MAX_PAGES = 0
            super(YahooEnum, self).__init__(base_url, self.engine_name, domain, subdomains, q=q, silent=silent, verbose=verbose)
            self.q = q
            return
    
        def extract_domains(self, resp):
            link_regx2 = re.compile('<span class=" fz-.*? fw-m fc-12th wr-bw.*?">(.*?)</span>')
            link_regx = re.compile('<span class="txt"><span class=" cite fw-xl fz-15px">(.*?)</span>')
            links_list = []
            try:
                links = link_regx.findall(resp)
                links2 = link_regx2.findall(resp)
                links_list = links + links2
                for link in links_list:
                    link = re.sub("<(\/)?b>", "", link)
                    if not link.startswith('http'):
                        link = "http://" + link
                    subdomain = urlparse.urlparse(link).netloc
                    if not subdomain.endswith(self.domain):
                        continue
                    if subdomain and subdomain not in self.subdomains and subdomain != self.domain:
                        if self.verbose:
                            self.print_("%s%s: %s%s" % (R, self.engine_name, W, subdomain))
                        self.subdomains.append(subdomain.strip())
            except Exception:
                pass
    
            return links_list
    
        def should_sleep(self):
            return
    
        def get_page(self, num):
            return num + 10
    
        def generate_query(self):
            if self.subdomains:
                fmt = 'site:{domain} -domain:www.{domain} -domain:{found}'
                found = ' -domain:'.join(self.subdomains[:77])
                query = fmt.format(domain=self.domain, found=found)
            else:
                query = "site:{domain}".format(domain=self.domain)
            return query
    
    
    class AskEnum(enumratorBaseThreaded):
        def __init__(self, domain, subdomains=None, q=None, silent=False, verbose=True):
            subdomains = subdomains or []
            base_url = 'http://www.ask.com/web?q={query}&page={page_no}&qid=8D6EE6BF52E0C04527E51F64F22C4534&o=0&l=dir&qsrc=998&qo=pagination'
            self.engine_name = "Ask"
            self.MAX_DOMAINS = 11
            self.MAX_PAGES = 0
            enumratorBaseThreaded.__init__(self, base_url, self.engine_name, domain, subdomains, q=q, silent=silent, verbose=verbose)
            self.q = q
            return
    
        def extract_domains(self, resp):
            links_list = list()
            link_regx = re.compile('<p class="web-result-url">(.*?)</p>')
            try:
                links_list = link_regx.findall(resp)
                for link in links_list:
                    if not link.startswith('http'):
                        link = "http://" + link
                    subdomain = urlparse.urlparse(link).netloc
                    if subdomain not in self.subdomains and subdomain != self.domain:
                        if self.verbose:
                            self.print_("%s%s: %s%s" % (R, self.engine_name, W, subdomain))
                        self.subdomains.append(subdomain.strip())
            except Exception:
                pass
    
            return links_list
    
        def get_page(self, num):
            return num + 1
    
        def generate_query(self):
            if self.subdomains:
                fmt = 'site:{domain} -www.{domain} -{found}'
                found = ' -'.join(self.subdomains[:self.MAX_DOMAINS])
                query = fmt.format(domain=self.domain, found=found)
            else:
                query = "site:{domain} -www.{domain}".format(domain=self.domain)
    
            return query
    
    
    class BingEnum(enumratorBaseThreaded):
        def __init__(self, domain, subdomains=None, q=None, silent=False, verbose=True):
            subdomains = subdomains or []
            base_url = 'https://www.bing.com/search?q={query}&go=Submit&first={page_no}'
            self.engine_name = "Bing"
            self.MAX_DOMAINS = 30
            self.MAX_PAGES = 0
            enumratorBaseThreaded.__init__(self, base_url, self.engine_name, domain, subdomains, q=q, silent=silent)
            self.q = q
            self.verbose = verbose
            return
    
        def extract_domains(self, resp):
            links_list = list()
            link_regx = re.compile('<li class="b_algo"><h2><a href="(.*?)"')
            link_regx2 = re.compile('<div class="b_title"><h2><a href="(.*?)"')
            try:
                links = link_regx.findall(resp)
                links2 = link_regx2.findall(resp)
                links_list = links + links2
    
                for link in links_list:
                    link = re.sub('<(\/)?strong>|<span.*?>|<|>', '', link)
                    if not link.startswith('http'):
                        link = "http://" + link
                    subdomain = urlparse.urlparse(link).netloc
                    if subdomain not in self.subdomains and subdomain != self.domain:
                        if self.verbose:
                            self.print_("%s%s: %s%s" % (R, self.engine_name, W, subdomain))
                        self.subdomains.append(subdomain.strip())
            except Exception:
                pass
    
            return links_list
    
        def generate_query(self):
            if self.subdomains:
                fmt = 'domain:{domain} -www.{domain} -{found}'
                found = ' -'.join(self.subdomains[:self.MAX_DOMAINS])
                query = fmt.format(domain=self.domain, found=found)
            else:
                query = "domain:{domain} -www.{domain}".format(domain=self.domain)
            return query
    
    
    class BaiduEnum(enumratorBaseThreaded):
        def __init__(self, domain, subdomains=None, q=None, silent=False, verbose=True):
            subdomains = subdomains or []
            base_url = 'https://www.baidu.com/s?pn={page_no}&wd={query}&oq={query}'
            self.engine_name = "Baidu"
            self.MAX_DOMAINS = 2
            self.MAX_PAGES = 760
            enumratorBaseThreaded.__init__(self, base_url, self.engine_name, domain, subdomains, q=q, silent=silent, verbose=verbose)
            self.querydomain = self.domain
            self.q = q
            return
    
        def extract_domains(self, resp):
            links = list()
            found_newdomain = False
            subdomain_list = []
            link_regx = re.compile('<a.*?class="c-showurl".*?>(.*?)</a>')
            try:
                links = link_regx.findall(resp)
                for link in links:
                    link = re.sub('<.*?>|>|<|&nbsp;', '', link)
                    if not link.startswith('http'):
                        link = "http://" + link
                    subdomain = urlparse.urlparse(link).netloc
                    if subdomain.endswith(self.domain):
                        subdomain_list.append(subdomain)
                        if subdomain not in self.subdomains and subdomain != self.domain:
                            found_newdomain = True
                            if self.verbose:
                                self.print_("%s%s: %s%s" % (R, self.engine_name, W, subdomain))
                            self.subdomains.append(subdomain.strip())
            except Exception:
                pass
            if not found_newdomain and subdomain_list:
                self.querydomain = self.findsubs(subdomain_list)
            return links
    
        def findsubs(self, subdomains):
            count = Counter(subdomains)
            subdomain1 = max(count, key=count.get)
            count.pop(subdomain1, "None")
            subdomain2 = max(count, key=count.get) if count else ''
            return (subdomain1, subdomain2)
    
        def check_response_errors(self, resp):
            return True
    
        def should_sleep(self):
            time.sleep(random.randint(2, 5))
            return
    
        def generate_query(self):
            if self.subdomains and self.querydomain != self.domain:
                found = ' -site:'.join(self.querydomain)
                query = "site:{domain} -site:www.{domain} -site:{found} ".format(domain=self.domain, found=found)
            else:
                query = "site:{domain} -site:www.{domain}".format(domain=self.domain)
            return query
    
    
    class NetcraftEnum(enumratorBaseThreaded):
        def __init__(self, domain, subdomains=None, q=None, silent=False, verbose=True):
            subdomains = subdomains or []
            self.base_url = 'https://searchdns.netcraft.com/?restriction=site+ends+with&host={domain}'
            self.engine_name = "Netcraft"
            self.lock = threading.Lock()
            super(NetcraftEnum, self).__init__(self.base_url, self.engine_name, domain, subdomains, q=q, silent=silent, verbose=verbose)
            self.q = q
            return
    
        def req(self, url, cookies=None):
            cookies = cookies or {}
            try:
                resp = self.session.get(url, headers=self.headers, timeout=self.timeout, cookies=cookies)
            except Exception as e:
                self.print_(e)
                resp = None
            return resp
    
        def get_next(self, resp):
            link_regx = re.compile('<A href="(.*?)"><b>Next page</b></a>')
            link = link_regx.findall(resp)
            link = re.sub('host=.*?%s' % self.domain, 'host=%s' % self.domain, link[0])
            url = 'http://searchdns.netcraft.com' + link
            return url
    
        def create_cookies(self, cookie):
            cookies = dict()
            cookies_list = cookie[0:cookie.find(';')].split("=")
            cookies[cookies_list[0]] = cookies_list[1]
            # hashlib.sha1 requires utf-8 encoded str
            cookies['netcraft_js_verification_response'] = hashlib.sha1(urllib.unquote(cookies_list[1]).encode('utf-8')).hexdigest()
            return cookies
    
        def get_cookies(self, headers):
            if 'set-cookie' in headers:
                cookies = self.create_cookies(headers['set-cookie'])
            else:
                cookies = {}
            return cookies
    
        def enumerate(self):
            start_url = self.base_url.format(domain='example.com')
            resp = self.req(start_url)
            cookies = self.get_cookies(resp.headers)
            url = self.base_url.format(domain=self.domain)
            while True:
                resp = self.get_response(self.req(url, cookies))
                self.extract_domains(resp)
                if 'Next page' not in resp:
                    return self.subdomains
                    break
                url = self.get_next(resp)
    
        def extract_domains(self, resp):
            links_list = list()
            link_regx = re.compile('<a href="http://toolbar.netcraft.com/site_report\?url=(.*)">')
            try:
                links_list = link_regx.findall(resp)
                for link in links_list:
                    subdomain = urlparse.urlparse(link).netloc
                    if not subdomain.endswith(self.domain):
                        continue
                    if subdomain and subdomain not in self.subdomains and subdomain != self.domain:
                        if self.verbose:
                            self.print_("%s%s: %s%s" % (R, self.engine_name, W, subdomain))
                        self.subdomains.append(subdomain.strip())
            except Exception:
                pass
            return links_list
    
    
    class DNSdumpster(enumratorBaseThreaded):
        def __init__(self, domain, subdomains=None, q=None, silent=False, verbose=True):
            subdomains = subdomains or []
            base_url = 'https://dnsdumpster.com/'
            self.live_subdomains = []
            self.engine_name = "DNSdumpster"
            self.threads = 70
            self.lock = threading.BoundedSemaphore(value=self.threads)
            self.q = q
            super(DNSdumpster, self).__init__(base_url, self.engine_name, domain, subdomains, q=q, silent=silent, verbose=verbose)
            return
    
        def check_host(self, host):
            is_valid = False
            Resolver = dns.resolver.Resolver()
            Resolver.nameservers = ['8.8.8.8', '8.8.4.4']
            self.lock.acquire()
            try:
                ip = Resolver.query(host, 'A')[0].to_text()
                if ip:
                    if self.verbose:
                        self.print_("%s%s: %s%s" % (R, self.engine_name, W, host))
                    is_valid = True
                    self.live_subdomains.append(host)
            except:
                pass
            self.lock.release()
            return is_valid
    
        def req(self, req_method, url, params=None):
            params = params or {}
            headers = dict(self.headers)
            headers['Referer'] = 'https://dnsdumpster.com'
            try:
                if req_method == 'GET':
                    resp = self.session.get(url, headers=headers, timeout=self.timeout)
                else:
                    resp = self.session.post(url, data=params, headers=headers, timeout=self.timeout)
            except Exception as e:
                self.print_(e)
                resp = None
            return self.get_response(resp)
    
        def get_csrftoken(self, resp):
            csrf_regex = re.compile("<input type='hidden' name='csrfmiddlewaretoken' value='(.*?)' />", re.S)
            token = csrf_regex.findall(resp)[0]
            return token.strip()
    
        def enumerate(self):
            resp = self.req('GET', self.base_url)
            token = self.get_csrftoken(resp)
            params = {'csrfmiddlewaretoken': token, 'targetip': self.domain}
            post_resp = self.req('POST', self.base_url, params)
            self.extract_domains(post_resp)
            for subdomain in self.subdomains:
                t = threading.Thread(target=self.check_host, args=(subdomain,))
                t.start()
                t.join()
            return self.live_subdomains
    
        def extract_domains(self, resp):
            tbl_regex = re.compile('<a name="hostanchor"><\/a>Host Records.*?<table.*?>(.*?)</table>', re.S)
            link_regex = re.compile('<td class="col-md-4">(.*?)<br>', re.S)
            links = []
            try:
                results_tbl = tbl_regex.findall(resp)[0]
            except IndexError:
                results_tbl = ''
            links_list = link_regex.findall(results_tbl)
            links = list(set(links_list))
            for link in links:
                subdomain = link.strip()
                if not subdomain.endswith(self.domain):
                    continue
                if subdomain and subdomain not in self.subdomains and subdomain != self.domain:
                    self.subdomains.append(subdomain.strip())
            return links
    
    
    class Virustotal(enumratorBaseThreaded):
        def __init__(self, domain, subdomains=None, q=None, silent=False, verbose=True):
            subdomains = subdomains or []
            base_url = 'https://www.virustotal.com/en/domain/{domain}/information/'
            self.engine_name = "Virustotal"
            self.lock = threading.Lock()
            self.q = q
            super(Virustotal, self).__init__(base_url, self.engine_name, domain, subdomains, q=q, silent=silent, verbose=verbose)
            return
    
        # the main send_req need to be rewritten
        def send_req(self, url):
            try:
                resp = self.session.get(url, headers=self.headers, timeout=self.timeout)
            except Exception as e:
                self.print_(e)
                resp = None
    
            return self.get_response(resp)
    
        # once the send_req is rewritten we don't need to call this function, the stock one should be ok
        def enumerate(self):
            url = self.base_url.format(domain=self.domain)
            resp = self.send_req(url)
            self.extract_domains(resp)
            return self.subdomains
    
        def extract_domains(self, resp):
            link_regx = re.compile('<div class="enum.*?">.*?<a target="_blank" href=".*?">(.*?)</a>', re.S)
            try:
                links = link_regx.findall(resp)
                for link in links:
                    subdomain = link.strip()
                    if not subdomain.endswith(self.domain):
                        continue
                    if subdomain not in self.subdomains and subdomain != self.domain:
                        if self.verbose:
                            self.print_("%s%s: %s%s" % (R, self.engine_name, W, subdomain))
                        self.subdomains.append(subdomain.strip())
            except Exception:
                pass
    
    
    class ThreatCrowd(enumratorBaseThreaded):
        def __init__(self, domain, subdomains=None, q=None, silent=False, verbose=True):
            subdomains = subdomains or []
            base_url = 'https://www.threatcrowd.org/searchApi/v2/domain/report/?domain={domain}'
            self.engine_name = "ThreatCrowd"
            self.lock = threading.Lock()
            self.q = q
            super(ThreatCrowd, self).__init__(base_url, self.engine_name, domain, subdomains, q=q, silent=silent, verbose=verbose)
            return
    
        def req(self, url):
            try:
                resp = self.session.get(url, headers=self.headers, timeout=self.timeout)
            except Exception:
                resp = None
    
            return self.get_response(resp)
    
        def enumerate(self):
            url = self.base_url.format(domain=self.domain)
            resp = self.req(url)
            self.extract_domains(resp)
            return self.subdomains
    
        def extract_domains(self, resp):
            try:
                links = json.loads(resp)['subdomains']
                for link in links:
                    subdomain = link.strip()
                    if not subdomain.endswith(self.domain):
                        continue
                    if subdomain not in self.subdomains and subdomain != self.domain:
                        if self.verbose:
                            self.print_("%s%s: %s%s" % (R, self.engine_name, W, subdomain))
                        self.subdomains.append(subdomain.strip())
            except Exception as e:
                pass
    
    
    class CrtSearch(enumratorBaseThreaded):
        def __init__(self, domain, subdomains=None, q=None, silent=False, verbose=True):
            subdomains = subdomains or []
            base_url = 'https://crt.sh/?q=%25.{domain}'
            self.engine_name = "SSL Certificates"
            self.lock = threading.Lock()
            self.q = q
            super(CrtSearch, self).__init__(base_url, self.engine_name, domain, subdomains, q=q, silent=silent, verbose=verbose)
            return
    
        def req(self, url):
            try:
                resp = self.session.get(url, headers=self.headers, timeout=self.timeout)
            except Exception:
                resp = None
    
            return self.get_response(resp)
    
        def enumerate(self):
            url = self.base_url.format(domain=self.domain)
            resp = self.req(url)
            if resp:
                self.extract_domains(resp)
            return self.subdomains
    
        def extract_domains(self, resp):
            link_regx = re.compile('<TD>(.*?)</TD>')
            try:
                links = link_regx.findall(resp)
                for link in links:
                    subdomain = link.strip()
                    if not subdomain.endswith(self.domain) or '*' in subdomain:
                        continue
    
                    if '@' in subdomain:
                        subdomain = subdomain[subdomain.find('@')+1:]
    
                    if subdomain not in self.subdomains and subdomain != self.domain:
                        if self.verbose:
                            self.print_("%s%s: %s%s" % (R, self.engine_name, W, subdomain))
                        self.subdomains.append(subdomain.strip())
            except Exception as e:
                pass
    
    
    class PassiveDNS(enumratorBaseThreaded):
        def __init__(self, domain, subdomains=None, q=None, silent=False, verbose=True):
            subdomains = subdomains or []
            base_url = 'https://api.sublist3r.com/search.php?domain={domain}'
            self.engine_name = "PassiveDNS"
            self.lock = threading.Lock()
            self.q = q
            super(PassiveDNS, self).__init__(base_url, self.engine_name, domain, subdomains, q=q, silent=silent, verbose=verbose)
            return
    
        def req(self, url):
            try:
                resp = self.session.get(url, headers=self.headers, timeout=self.timeout)
            except Exception as e:
                resp = None
    
            return self.get_response(resp)
    
        def enumerate(self):
            url = self.base_url.format(domain=self.domain)
            resp = self.req(url)
            if not resp:
                return self.subdomains
    
            self.extract_domains(resp)
            return self.subdomains
    
        def extract_domains(self, resp):
            try:
                subdomains = json.loads(resp)
                for subdomain in subdomains:
                    if subdomain not in self.subdomains and subdomain != self.domain:
                        if self.verbose:
                            self.print_("%s%s: %s%s" % (R, self.engine_name, W, subdomain))
                        self.subdomains.append(subdomain.strip())
            except Exception as e:
                pass
    
    
    class portscan():
        def __init__(self, subdomains, ports):
            self.subdomains = subdomains
            self.ports = ports
            self.threads = 20
            self.lock = threading.BoundedSemaphore(value=self.threads)
    
        def port_scan(self, host, ports):
            openports = []
            self.lock.acquire()
            for port in ports:
                try:
                    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
                    s.settimeout(2)
                    result = s.connect_ex((host, int(port)))
                    if result == 0:
                        openports.append(port)
                    s.close()
                except Exception:
                    pass
            self.lock.release()
            if len(openports) > 0:
                print("%s%s%s - %sFound open ports:%s %s%s%s" % (G, host, W, R, W, Y, ', '.join(openports), W))
    
        def run(self):
            for subdomain in self.subdomains:
                t = threading.Thread(target=self.port_scan, args=(subdomain, self.ports))
                t.start()
    
    
    def main(domain, threads, savefile, ports, silent, verbose, enable_bruteforce, engines):
        bruteforce_list = set()
        search_list = set()
    
        if is_windows:
            subdomains_queue = list()
        else:
            subdomains_queue = multiprocessing.Manager().list()
    
        # Check Bruteforce Status
        if enable_bruteforce or enable_bruteforce is None:
            enable_bruteforce = True
    
        # Validate domain
        domain_check = re.compile("^(http|https)?[a-zA-Z0-9]+([\-\.]{1}[a-zA-Z0-9]+)*\.[a-zA-Z]{2,}$")
        if not domain_check.match(domain):
            if not silent:
                print(R + "Error: Please enter a valid domain" + W)
            return []
    
        if not domain.startswith('http://') or not domain.startswith('https://'):
            domain = 'http://' + domain
    
        parsed_domain = urlparse.urlparse(domain)
    
        if not silent:
            print(B + "[-] Enumerating subdomains now for %s" % parsed_domain.netloc + W)
    
        if verbose and not silent:
            print(Y + "[-] verbosity is enabled, will show the subdomains results in realtime" + W)
    
        supported_engines = {'baidu': BaiduEnum,
                             'yahoo': YahooEnum,
                             'google': GoogleEnum,
                             'bing': BingEnum,
                             'ask': AskEnum,
                             'netcraft': NetcraftEnum,
                             'dnsdumpster': DNSdumpster,
                             'virustotal': Virustotal,
                             'threatcrowd': ThreatCrowd,
                             'ssl': CrtSearch,
                             'passivedns': PassiveDNS
                             }
    
        chosenEnums = []
    
        if engines is None:
            chosenEnums = [
                BaiduEnum, YahooEnum, GoogleEnum, BingEnum, AskEnum,
                NetcraftEnum, DNSdumpster, Virustotal, ThreatCrowd,
                CrtSearch, PassiveDNS
            ]
        else:
            engines = engines.split(',')
            for engine in engines:
                if engine.lower() in supported_engines:
                    chosenEnums.append(supported_engines[engine.lower()])
    
        # Start the engines enumeration
        enums = [enum(domain, [], q=subdomains_queue, silent=silent, verbose=verbose) for enum in chosenEnums]
        for enum in enums:
            enum.start()
        for enum in enums:
            enum.join()
    
        subdomains = set(subdomains_queue)
        for subdomain in subdomains:
            search_list.add(subdomain)
    
        if enable_bruteforce:
            if not silent:
                print(G + "[-] Starting bruteforce module now using subbrute.." + W)
            record_type = False
            path_to_file = os.path.dirname(os.path.realpath(__file__))
            subs = os.path.join(path_to_file, 'subbrute', 'names.txt')
            resolvers = os.path.join(path_to_file, 'subbrute', 'resolvers.txt')
            process_count = threads
            output = False
            json_output = False
            bruteforce_list = subbrute.print_target(parsed_domain.netloc, record_type, subs, resolvers, process_count, output, json_output, search_list, verbose)
    
        subdomains = search_list.union(bruteforce_list)
    
        if subdomains:
            subdomains = sorted(subdomains, key=subdomain_sorting_key)
    
            if savefile:
                write_file(savefile, subdomains)
    
            if not silent:
                print(Y + "[-] Total Unique Subdomains Found: %s" % len(subdomains) + W)
    
            if ports:
                if not silent:
                    print(G + "[-] Start port scan now for the following ports: %s%s" % (Y, ports) + W)
                ports = ports.split(',')
                pscan = portscan(subdomains, ports)
                pscan.run()
    
            elif not silent:
                for subdomain in subdomains:
                    print(G + subdomain + W)
        return subdomains
    
    
    def interactive():
        args = parse_args()
        domain = args.domain
        threads = args.threads
        savefile = args.output
        ports = args.ports
        enable_bruteforce = args.bruteforce
        verbose = args.verbose
        engines = args.engines
        if verbose or verbose is None:
            verbose = True
        banner()
        res = main(domain, threads, savefile, ports, silent=False, verbose=verbose, enable_bruteforce=enable_bruteforce, engines=engines)
    
    if __name__ == "__main__":
        interactive()
    